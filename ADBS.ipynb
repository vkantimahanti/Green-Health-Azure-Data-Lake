/*declaring the config variables to dynamically pick at run time*/
dbutils.widgets.text('username','')
username = dbutils.widgets.get('username')

dbutils.widgets.text('password','')
password = dbutils.widgets.get('password')

dbutils.widgets.text('directoryid','')
directoryid = dbutils.widgets.get('directoryid')

dbutils.widgets.text('Storagename','')
Storagename = dbutils.widgets.get('Storagename')

dbutils.widgets.text('mountpoint','')
mountpoint = dbutils.widgets.get('mountpoint')




#Mount the DataLake on Databricks file system

dbutils.fs.unmount("/mnt/"+mountpoint)

#Mounting Config
configs = {"fs.adl.oauth2.access.token.provider.type": "ClientCredential",
           "fs.adl.oauth2.client.id": username,
           "fs.adl.oauth2.credential": password,
           "fs.adl.oauth2.refresh.url": "https://login.microsoftonline.com/"+directoryid+"/oauth2/token"}

# Optionally, you can add <directory-name> to the source URI of your mount point.
dbutils.fs.mount(
  source = "adl://"+Storagename+".azuredatalakestore.net/"+mountpoint,
  mount_point = "/mnt/"+mountpoint,
  extra_configs = configs)
   
 
#setting source file paths dynamically
dbutils.widgets.text('sourcepath','')
sourcepath = dbutils.widgets.get('sourcepath')

dbutils.widgets.text('sourcefiletype','')
filetype = dbutils.widgets.get('sourcefiletype')

#setting target path to datalake 
dbutils.widgets.text('targetpath','')
sourcepath=dbutils.widgets.get('targetpath')

dbutils.widgets.text('targetfiletype','')
filetype=dbutils.widgets.get('targetfiletype')


#Check the files
%fs ls /mnt/SourceDataFiles/GreenHealth



#Path variables to run dynamically
FullFilePath_Claims = root_path+sourcepath+"/Claims.txt"
FullFilePath_Claims

FullFilePath_HealthInstitution = root_path+sourcepath+"/Health_Institution.txt"
FullFilePath_HealthInstitution


#Dataframes to validate the data
df=spark.read.format(filetype).option('header',True).load(FullFilePath_Claims)
display(df)


#Conver the non readable files to readable format by Reading data from Files to Data Frame
df1=spark.read.format(filetype).option('header',False).option("delimiter","\t").load(FullFilePath_HealthInstitution)
display(df1)

import pyspark.sql.functions as f

df_split = df1.select(f.split(df1._c0,"\\t")).rdd.flatMap(
              lambda x: x).toDF(schema=["col1","col2","col3","col4","col5","col6","col7","col8","col9"])			  
display(df_split)


df_split.write.format(filetype).mode("overwrite").option('header',True).save(root_path+sourcepath+"/Formated_Data_HealthInstitution/")
%fs ls /mnt/SourceDataFiles/GreenHealth/Formated_Data_HealthInstitution

df2=spark.read.format("csv").option('header',True).load("/mnt/SourceDataFiles/GreenHealth/Formated_Data_HealthInstitution/part-00000-tid-4958676402255233979-f8687a65-9695-4240-8c34-94ca4b0a3ef6-106-1-c000.csv")
display(df2)

FullFilePath_Member = root_path+sourcepath+"/Member.txt"
FullFilePath_Member

Member_df=spark.read.format(filetype).option('header',False).option("delimiter","\t").load(FullFilePath_Member)
display(Member_df)

Memdf_split = Member_df.select(f.split(Member_df._c0,"\\t")).rdd.flatMap(
              lambda x: x).toDF(schema=["col1","col2","col3","col4","col5","col6","col7","col8","col9", "col10","col11"])
			  
display(Memdf_split)

#converting tab delimited file to csv file and load to DataLake
Memdf_split.write.format(filetype).mode("overwrite").option('header',True).save(root_path+sourcepath+"/Formated_Data_Member/")			  


			  